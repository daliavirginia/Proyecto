{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0979c6",
   "metadata": {},
   "source": [
    "# ASCII para el MTM-SSA toolkit\n",
    "\n",
    "Script con el objetivo de crear un archivo ascii como datos de entrada para la aplicación de singlar spectrum analysis mediante el toolkit:\n",
    "https://dept.atmos.ucla.edu/tcd/ssa-mtm-toolkit\n",
    "\n",
    "En la sección Documentation > Demonstration se detalla que la serie temporal debe presentarse como tabla en un archivo ascii de la forma:\n",
    "\n",
    "| time | data |\n",
    "\n",
    "La serie temporal se obtiene a partir de datos de precipitación acumulada mensual de la base de datos GPCC. Los pasos a seguir son:\n",
    "1. Lectura de datos y recorte espacial enfocado en el centro de Argentina. \n",
    "2. Remuestreo de datos, de acumulados mensuales a estacionales. Se guarda todo por separado en un diccionario.\n",
    "3. Calculo de anomalías respecto del período 1951-1980\n",
    "4. Promedio regional con peso según latitud (se van las dimensiones lat y lon, por lo que queda una serie 1D)\n",
    "5. Se pasa de xarray.DataArray a pandas.Dataframe\n",
    "\n",
    "Luego se crea una tabla ascii (un csv por estación) usando la funcion to_csv() de pandas. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86befd",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdc7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188d321",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc575b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para serie temporal de anomalias CENTRO ARGENTINA\n",
    "LAT_SUR_M=-35\n",
    "LAT_NOR_M=-26\n",
    "LON_OESTE_M=294\n",
    "LON_ESTE_M=299\n",
    "\n",
    "#Para calculos CENTRO ARGENTINA\n",
    "LAT_SUR=-42\n",
    "LAT_NOR=-16\n",
    "LON_OESTE=287\n",
    "LON_ESTE=306\n",
    "\n",
    "#Periodo largo\n",
    "ANIO_MIN1=1930\n",
    "ANIO_MAX1=2016\n",
    "\n",
    "#Periodo corto\n",
    "ANIO_MIN2=1981\n",
    "ANIO_MAX2=2010\n",
    "\n",
    "#Periodo en formato\n",
    "TIMEMIN1 = str(ANIO_MIN1)+'-01-01'\n",
    "TIMEMAX1 = str(ANIO_MAX1)+'-12-31'\n",
    "\n",
    "#Periodo en formato\n",
    "TIMEMIN2 = str(ANIO_MIN2)+'-01-01'\n",
    "TIMEMAX2 = str(ANIO_MAX2)+'-12-31'\n",
    "\n",
    "#Periodo en el cual se calculará la media para las anomalías\n",
    "TIMEMIN_MEDIA = '1951-01-01'\n",
    "TIMEMAX_MEDIA = '1980-12-31'\n",
    "\n",
    "#Path a los datos\n",
    "DATOS_PP_GPCC='/datos/Datos_Dalia/precip.mon.total.v2018.nc'\n",
    "\n",
    "#Path para guardar figuras\n",
    "SALIDAS='/home/dalia.panza/Proy_IAI/Salidas/SSA/'\n",
    "\n",
    "#Vector con nombres de las estaciones del año\n",
    "KEYS=['Verano (DJF)', 'Otoño (MAM)', 'Invierno (JJA)', 'Primavera (SON)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9305d3",
   "metadata": {},
   "source": [
    "## Selección de período"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9db0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "per=1\n",
    "\n",
    "if (per==1):\n",
    "    \n",
    "    TIMEMIN=TIMEMIN1\n",
    "    TIMEMAX=TIMEMAX1\n",
    "    PER='(1930-2016)'\n",
    "    \n",
    "else:\n",
    "    \n",
    "    TIMEMIN=TIMEMIN2\n",
    "    TIMEMAX=TIMEMAX2\n",
    "    PER='(1981-2010)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b7e92",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f1537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abro el archivo usando xarray\n",
    "ds_pp = xr.open_dataset(DATOS_PP_GPCC)\n",
    "\n",
    "#Recorte temporal y espacial\n",
    "pp_recorte = ds_pp.sel(lat=slice(LAT_NOR, LAT_SUR), lon=slice(LON_OESTE,LON_ESTE))['precip']\n",
    "\n",
    "del ds_pp\n",
    "\n",
    "#Remuestreo los datos dividiendo por estaciones (DEF-MAM-JJA-SON)\n",
    "pp_est = pp_recorte.resample(time=\"QS-DEC\").sum()\n",
    "\n",
    "pp_est={KEYS[0]:pp_est.sel(time=pp_est[\"time.month\"]==12),\n",
    "        KEYS[1]:pp_est.sel(time=pp_est[\"time.month\"]==3),\n",
    "        KEYS[2]:pp_est.sel(time=pp_est[\"time.month\"]==6),\n",
    "        KEYS[3]:pp_est.sel(time=pp_est[\"time.month\"]==9)}\n",
    "\n",
    "#Excluyo el primer y ultimo datos de verano (Tienen meses creados que no estan en los datos)\n",
    "pp_est[KEYS[0]]=pp_est[KEYS[0]].sel(time=slice(pp_est[KEYS[0]]['time'][1],\n",
    "                                                pp_est[KEYS[0]]['time'][-2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d738e9",
   "metadata": {},
   "source": [
    "## Promedio regional de anomalías (3D --> 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21d68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########CALCULO DE ANOMALÍAS############\n",
    "\n",
    "#Anomalía para cada punto de reticula\n",
    "\n",
    "\"\"\"Para la serie original\"\"\"\n",
    "pp_media = pp_recorte.sel(lat=slice(LAT_NOR_M,LAT_SUR_M),\n",
    "                               lon=slice(LON_OESTE_M,LON_ESTE_M),\n",
    "                               time=slice(TIMEMIN_MEDIA,TIMEMAX_MEDIA)).mean()\n",
    "pp_anom =pp_recorte.sel(time=slice(TIMEMIN,TIMEMAX)) - pp_media\n",
    "\n",
    "\"\"\"Por estación\"\"\"\n",
    "#Inicializo lista vacia \n",
    "pp_est_anom=[]\n",
    "\n",
    "#Ciclo para iterar en el diccionario de datos de pp\n",
    "for key in KEYS:\n",
    "    #Hago un recorte espacial para el centro de Arg y el periodo 51-80\n",
    "    pp_media = pp_est[key].sel(lat=slice(LAT_NOR_M,LAT_SUR_M),\n",
    "                               lon=slice(LON_OESTE_M,LON_ESTE_M),\n",
    "                               time=slice(TIMEMIN_MEDIA,TIMEMAX_MEDIA)).mean()\n",
    "    #A la lista vacía le agrego la anomalía para la estación correspondiente\n",
    "    pp_est_anom.append(pp_est[key].sel(time=slice(TIMEMIN,TIMEMAX))-pp_media)\n",
    "    \n",
    "    del pp_media\n",
    "\n",
    "#Paso de list a dict \n",
    "pp_est_anom= dict(zip(KEYS, pp_est_anom))\n",
    "\n",
    "\"\"\"Anual\"\"\"\n",
    "pp_ann_anom=pp_anom.resample(time=\"Y\").sum()\n",
    "\n",
    "############PROMEDIO REGIONAL################\n",
    "\n",
    "#Promedio regional con pesos segun latitud\n",
    "weights = np.cos(np.deg2rad(pp_recorte.lat))\n",
    "\n",
    "\"\"\"Para la serie original\"\"\"\n",
    "pp_anom_PromReg = pp_anom.sel(lat=slice(LAT_NOR_M,LAT_SUR_M),\n",
    "                          lon=slice(LON_OESTE_M,LON_ESTE_M)).weighted(weights).mean((\"lon\", \"lat\"))\n",
    "pp_anom_PromReg = pp_anom_PromReg.to_dataframe()\n",
    "\"\"\"Por estación\"\"\"\n",
    "#Inicializo lista vacía\n",
    "pp_anom_est_PromReg=[]\n",
    "\n",
    "#Data array con valores entre 0 y 1 para hacer promedio con peso x latitud\n",
    "\n",
    "for key in KEYS:\n",
    "    da_weighted = pp_est_anom[key].sel(lat=slice(LAT_NOR_M,LAT_SUR_M),\n",
    "                                      lon=slice(LON_OESTE_M,LON_ESTE_M)).weighted(weights)\n",
    "    pp_anom_est_PromReg.append(da_weighted.mean((\"lon\", \"lat\")))\n",
    "\n",
    "                      \n",
    "#Paso de list a dict\n",
    "pp_anom_est_PromReg= dict(zip(KEYS, pp_anom_est_PromReg))\n",
    "\n",
    "#del pp_recorte, ds_weighted, pp_est_anom, pp_est\n",
    "\n",
    "#Paso los DataArrays a pandas DataFrames\n",
    "for key in KEYS:\n",
    "    pp_anom_est_PromReg[key] = pp_anom_est_PromReg[key].to_dataframe()\n",
    "\n",
    "\n",
    "\"\"\"Anual\"\"\"\n",
    "pp_ann_anom_PromReg = pp_ann_anom.sel(lat=slice(LAT_NOR_M,LAT_SUR_M),\n",
    "                          lon=slice(LON_OESTE_M,LON_ESTE_M)).weighted(weights).mean((\"lon\", \"lat\"))\n",
    "\n",
    "pp_ann_anom_PromReg = pp_ann_anom_PromReg.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad92385",
   "metadata": {},
   "source": [
    "## Creación de ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf09878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "1930-12-01    103.963638\n",
      "1931-12-01     19.145315\n",
      "1932-12-01    -36.733856\n",
      "1933-12-01   -117.120476\n",
      "1934-12-01    -67.323784\n",
      "                 ...    \n",
      "2011-12-01    -68.633232\n",
      "2012-12-01    -82.130913\n",
      "2013-12-01     41.520748\n",
      "2014-12-01    126.656120\n",
      "2015-12-01     88.353111\n",
      "Name: precip, Length: 86, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print(pp_anom_est_PromReg[KEYS[0]]['precip'])\n",
    "\n",
    "for key in KEYS:\n",
    "    \n",
    "    path = SALIDAS + 'ascii_ssa_toolkit_{}.csv'.format(key[-4:-1])\n",
    "   \n",
    "    pp_anom_est_PromReg[key]['precip'].to_csv(path, header=False, encoding='ascii', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f742f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
